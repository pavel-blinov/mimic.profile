{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please verify that you have installed the python libraries from requirements.txt\n",
    "\n",
    "## You can install them with ```pip install -r requirements.txt```\n",
    "## Other versions may work but they are not tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# set desired gpu device\n",
    "import os; os.environ['CUDA_VISIBLE_DEVICES'] = '0';\n",
    "import gc\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "import random\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import model_selection\n",
    "from scipy.special import expit\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers.optimization import AdamW\n",
    "from transformers import RobertaConfig\n",
    "\n",
    "from utils import processing_mimic3, get_score, RobertaForICD9, RobertaForICD9_wo_positional\n",
    "\n",
    "SEED = 79\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to the correct path to MIMIC3 data files, e.g. '/ayb/vol3/datasets/MIMIC3/'\n",
    "MIMIC_PATH = '?'\n",
    "# this is the name of preprocessed file\n",
    "MIMIC_data_file = 'patients_mimic3_full.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## prepare data file for all patients from raw MIMIC3 data\n",
    "file_adm = os.path.join(MIMIC_PATH, 'ADMISSIONS.csv')\n",
    "file_dxx = os.path.join(MIMIC_PATH, 'DIAGNOSES_ICD.csv')\n",
    "file_txx = os.path.join(MIMIC_PATH, 'PROCEDURES_ICD.csv')\n",
    "file_drug = os.path.join(MIMIC_PATH, 'PRESCRIPTIONS.csv')\n",
    "file_drg = os.path.join(MIMIC_PATH, 'DRGCODES.csv')\n",
    "\n",
    "processing_mimic3(file_adm, file_dxx, file_txx, file_drug, file_drg, MIMIC_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment if you would like to download the file AppendixCMultiDX.txt from the original source\n",
    "# !wget -c --tries=0 --read-timeout=10 \"https://www.hcup-us.ahrq.gov/toolssoftware/ccs/AppendixCMultiDX.txt\" -O AppendixCMultiDX.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build map 'icd9 code' -> 'second level category' from ICD9 hierarchy\n",
    "with open('AppendixCMultiDX.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "categ = ''\n",
    "code_lines = []\n",
    "cat2line = {}\n",
    "for line in lines:\n",
    "    is_name_line = line[0].isdigit()\n",
    "    if is_name_line: # start of a new categ\n",
    "        cat2line[categ] = code_lines\n",
    "        \n",
    "        categ = line\n",
    "        \n",
    "        parts = line.split(' ')\n",
    "        index = parts[0]\n",
    "        categ = index\n",
    "        \n",
    "        code_lines = []\n",
    "    else:\n",
    "        code_lines.append(line)\n",
    "cat2line[categ] = code_lines\n",
    "\n",
    "fcat2codes = {} #full category to code\n",
    "for cat in cat2line:\n",
    "    if len(cat)>0:\n",
    "        codes = set()\n",
    "        for line in cat2line[cat]:\n",
    "            line = re.sub(' +', ' ', line).strip()\n",
    "            if len(line)>0:\n",
    "                subset = line.split(' ')\n",
    "                codes.update( subset )\n",
    "        if len(codes)>0:\n",
    "            fcat2codes[cat] = codes\n",
    "            \n",
    "unique_categs = set() # unique second level categories\n",
    "for i in fcat2codes.keys():\n",
    "    parts = i.split('.')\n",
    "    if len(parts)>0:\n",
    "        cat = '.'.join(parts[:2])\n",
    "        unique_categs.add(cat)\n",
    "        \n",
    "cat2codes = {}\n",
    "for i, cat in enumerate(sorted(list(unique_categs))):\n",
    "    codes = set()\n",
    "    for key in fcat2codes:\n",
    "        trimed_key = '.'.join(key.split('.')[:2])\n",
    "        if trimed_key==cat:\n",
    "            subset = fcat2codes[key]\n",
    "            codes.update( subset )\n",
    "    cat2codes[i] = sorted(list(codes))\n",
    "    \n",
    "code2categ = {}\n",
    "for cat in cat2codes:\n",
    "    codes = cat2codes[cat]\n",
    "    for code in codes:\n",
    "        code2categ[code] = cat\n",
    "print (len(code2categ))\n",
    "\n",
    "num_categs = len(set(code2categ.values()))\n",
    "\n",
    "print (num_categs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## len(code2categ) == 15072\n",
    "## num_categs == 136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set params to filter out histories and diagnoses\n",
    "min_visits_number = 2\n",
    "infrequent_diagnoses = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open(MIMIC_data_file) as f:\n",
    "    jdata = json.load(f)\n",
    "print (len(jdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnoses = []\n",
    "for rec in jdata:\n",
    "    for v in rec['visits']:\n",
    "        diagnoses.extend( v['DXs'] )\n",
    "diagnoses = dict(Counter(diagnoses))\n",
    "\n",
    "left_diagnoses = set([code for code in diagnoses if diagnoses[code] >= infrequent_diagnoses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_patients = []\n",
    "\n",
    "for rec in jdata:\n",
    "    if len(rec['visits']) < min_visits_number:\n",
    "        continue\n",
    "    \n",
    "    ok_visit = 0\n",
    "    for v in rec['visits']:\n",
    "        current_DXs = set( v['DXs'] )\n",
    "        leftover = current_DXs.intersection(left_diagnoses)\n",
    "        if len(leftover)>0:\n",
    "            ok_visit += 1\n",
    "    \n",
    "    if ok_visit >= min_visits_number:\n",
    "        selected_patients.append(rec)\n",
    "selected_patients = np.array(selected_patients)\n",
    "print (len(selected_patients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_visits = np.mean([len(rec['visits']) for rec in selected_patients])\n",
    "print (mean_visits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## len(selected_patients) == 7496\n",
    "## mean_visits == 2.6562166488794023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv( os.path.join(MIMIC_PATH, 'PATIENTS.csv') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid2gender = {str(pid):1 if g=='M' else 0 for pid, g in zip(df.SUBJECT_ID, df.GENDER)}\n",
    "pid2dob = {str(pid):int(time.mktime(datetime.datetime.strptime(g.split(' ')[0], \"%Y-%m-%d\").timetuple())) for pid, g in zip(df.SUBJECT_ID, df.DOB)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# recs is array of tuples (gender, age, history_codes, the_last_visit_codes)\n",
    "recs = []\n",
    "for rec in selected_patients:\n",
    "    pid = rec['pid']\n",
    "    gender = pid2gender[pid]\n",
    "    \n",
    "    #sort visits by time\n",
    "    timestamps = []\n",
    "    for v in rec['visits']:\n",
    "        admsn_dt = v['admsn_dt']\n",
    "        ts = int(time.mktime(datetime.datetime.strptime(admsn_dt, \"%Y%m%d\").timetuple()))\n",
    "        timestamps.append(ts)\n",
    "    order = np.argsort(timestamps)\n",
    "    \n",
    "    rec['visits'] = list( np.array(rec['visits'])[order] )\n",
    "    \n",
    "    history = [] #collect history up to last visit\n",
    "    for v in rec['visits'][:-1]:\n",
    "        for diag9 in v['DXs']:\n",
    "            history.append( diag9 )\n",
    "    \n",
    "    # compute the patient age at pre last visit\n",
    "    ts = int(time.mktime(datetime.datetime.strptime(v['admsn_dt'], \"%Y%m%d\").timetuple()))\n",
    "    age = relativedelta(datetime.datetime.fromtimestamp(ts), datetime.datetime.fromtimestamp(pid2dob[pid])).years\n",
    "    age = min(89, age) # to overcome obscure in MIMIC\n",
    "    \n",
    "    target = []\n",
    "    v = rec['visits'][-1]\n",
    "    for diag9 in v['DXs']:\n",
    "        target.append( diag9 )\n",
    "    \n",
    "    recs.append( (gender, age, history, target) )\n",
    "recs = np.array(recs, dtype=object)\n",
    "print (recs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of auxilary tokens is 4: 0(start), 1(pad), 2(end), 3(mask)\n",
    "#plus couple tokens for gender: 4th index for women, 5th index for men\n",
    "#plus token indicies for each reasonable age: 0-99\n",
    "code_shift = 4+\\\n",
    "2+\\\n",
    "100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_codes = []\n",
    "for rec in recs:\n",
    "    all_codes.extend(rec[2])\n",
    "    all_codes.extend(rec[3])\n",
    "all_codes = set(all_codes)\n",
    "print (len(all_codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code2index = {}\n",
    "counter = 0\n",
    "for c in sorted(list(all_codes)):\n",
    "    # ICD token indices starts after 99+{0/1}+4\n",
    "    code2index[c] = code_shift + counter\n",
    "    counter+=1\n",
    "print (len(code2index))\n",
    "\n",
    "index2code = {code2index[c]:c for c in code2index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_cases = 128 # consider the patient history no more than 'num_cases'==128 elemens\n",
    "\n",
    "# rec vectorizer funcition\n",
    "def vectorize(subset):\n",
    "    vectors, targets = [], []\n",
    "    for rec in subset:\n",
    "        #sex / age components\n",
    "        pref = [\n",
    "            0, #start sequence token\n",
    "            # 4 is the number of auxilary tokens\n",
    "            4 + rec[0], # 4th for women, 5th token for men\n",
    "            6 + rec[1] # token index for each posible age\n",
    "        ] # ang and sex prefix\n",
    "\n",
    "        history = np.ones( num_cases+1 )\n",
    "\n",
    "        # convert ICD codes to sequence of token indices\n",
    "        postf = []\n",
    "        for c in rec[2][:num_cases]:\n",
    "            if c in code2index:\n",
    "                postf.append( code2index[c] )\n",
    "        postf.append(2) # end sequence token\n",
    "\n",
    "        history[:len(postf)] = postf\n",
    "\n",
    "        history = np.concatenate([pref, history])\n",
    "\n",
    "        vectors.append(history)\n",
    "        \n",
    "        t_out = np.zeros(num_categs)\n",
    "        for c in rec[3]:\n",
    "            idx = code2categ[c]\n",
    "            t_out[idx] = 1\n",
    "        targets.append(t_out)\n",
    "    targets = np.array(targets)\n",
    "        \n",
    "    vectors = np.array(vectors)\n",
    "    return vectors, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 19\n",
    "lr = 9e-5\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "scores = []\n",
    "kf = model_selection.KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "for fold_num, (train_index, cv_index) in enumerate(kf.split(recs), start=1):\n",
    "    X_train, y_train = vectorize( recs[train_index] )    \n",
    "    X_cv, y_cv = vectorize( recs[cv_index] )\n",
    "    \n",
    "    vocab_size = np.max( list(code2index.values()) )\n",
    "    hidden_dim = 256\n",
    "    layer_and_heads = 4\n",
    "\n",
    "    config = RobertaConfig(\n",
    "        bos_token_id=0,\n",
    "        pad_token_id=1,\n",
    "        eos_token_id=2,\n",
    "\n",
    "        initializer_range=.02,\n",
    "\n",
    "        hidden_size=hidden_dim,\n",
    "        intermediate_size=hidden_dim*2,\n",
    "\n",
    "        max_position_embeddings=X_train.shape[1]+2,\n",
    "        num_attention_heads=layer_and_heads,\n",
    "        num_hidden_layers=layer_and_heads,\n",
    "        type_vocab_size=4,\n",
    "\n",
    "        vocab_size=vocab_size\n",
    "    )\n",
    "    \n",
    "    config.num_labels = num_categs\n",
    "    #print (config)\n",
    "    #model = RobertaForICD9(config=config)\n",
    "    model = RobertaForICD9_wo_positional(config=config)\n",
    "    #model.num_parameters()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    ##train_dataloader\n",
    "    input_ids_train = torch.tensor( X_train.astype(int) )\n",
    "    attention_masks_train = torch.tensor( (X_train!=1).astype(int) )\n",
    "    labels_train = torch.tensor( y_train )\n",
    "    train_data = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data,\n",
    "        sampler=train_sampler,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,\n",
    "        worker_init_fn=seed_worker\n",
    "    )\n",
    "\n",
    "    ##prediction_dataloader\n",
    "    input_ids_test = torch.tensor( X_cv.astype(int) )\n",
    "    attention_masks_test = torch.tensor( (X_cv!=1).astype(int) )\n",
    "    labels_test = torch.tensor( y_cv )\n",
    "    prediction_data = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
    "    prediction_sampler = SequentialSampler(prediction_data)\n",
    "    prediction_dataloader = DataLoader(\n",
    "        prediction_data,\n",
    "        sampler=prediction_sampler,\n",
    "        batch_size=batch_size*2,\n",
    "        num_workers=4,\n",
    "        worker_init_fn=seed_worker\n",
    "    )\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=lr, correct_bias=False)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "    train_loss = []\n",
    "    for _ in range(epochs):\n",
    "        model.train(); torch.cuda.empty_cache()\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "        # Train the data for one epoch\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            # Clear out the gradients (by default they accumulate)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model( b_input_ids, attention_mask=b_input_mask, labels=b_labels )\n",
    "            loss, logits = outputs[:2]\n",
    "            train_loss.append(loss.item())\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # Update parameters and take a step using the computed gradient\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update tracking variables\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "        tr_loss_value = tr_loss/nb_tr_steps\n",
    "\n",
    "        ### val\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        tr_loss, nb_tr_steps = 0, 0\n",
    "        for step, batch in enumerate(prediction_dataloader):\n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "            with torch.no_grad():\n",
    "                # Forward pass, calculate logit predictions\n",
    "                outputs = model( b_input_ids, attention_mask=b_input_mask, labels=b_labels )\n",
    "                loss, logits = outputs[:2]\n",
    "                tr_loss += loss.item()\n",
    "                nb_tr_steps += 1\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            # Store predictions and true labels\n",
    "            predictions.append(logits)\n",
    "        predictions = expit(np.vstack(predictions))\n",
    "        val_loss_value = tr_loss/nb_tr_steps\n",
    "        \n",
    "        scheduler.step(val_loss_value)\n",
    "        \n",
    "        score5 = get_score(y_cv, predictions)\n",
    "        \n",
    "        print ( 'P@5:{}\\tEPOCH {} train loss: {} val loss: {}'.format(score5, _, tr_loss_value, val_loss_value) )\n",
    "    \n",
    "    fold_scores = [get_score(y_cv, predictions, k=k) for k in [5, 10, 20, 30]]\n",
    "    scores.append(fold_scores)\n",
    "    print ('\\tFold {}: {}'.format(fold_num, fold_scores))\n",
    "\n",
    "scores = np.array(scores)\n",
    "precision_k_means = np.round(\n",
    "    np.mean(scores, axis=0),\n",
    "    decimals=4\n",
    ")\n",
    "\n",
    "precision_k_stds = np.round(\n",
    "    np.std(scores, axis=0),\n",
    "    decimals=4\n",
    ")\n",
    "\n",
    "print ('\\nPrecision@k')\n",
    "print ('means', precision_k_means)\n",
    "print ('stds', precision_k_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision@k\n",
    "# means [0.6677 0.619  0.7258 0.8217]\n",
    "# stds [0.0084 0.0099 0.0083 0.0045]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
